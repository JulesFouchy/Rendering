{"searchDocs":[{"title":"Pipeline de Rendu","type":0,"sectionRef":"#","url":"/Rendering/Deep dive","content":"Pipeline de Rendu Voici une vue d'ensemble de tout ce qui se passe dans Unreal : Pour faire plus simple, voici ce qu'il faut retenir sur les bases du rendu 3D en rasterisation : Que vous pouvez retrouver expliquées dans cette super vidéo :","keywords":"","version":"Next"},{"title":"Anti-Aliasing","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/Anti-Aliasing","content":"","keywords":"","version":"Next"},{"title":"Qu'est-ce que l'aliasing ?​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#quest-ce-que-laliasing-","content":" Quand vous essayez de dessiner une ligne en diagonale, elle ne peut pas aller &quot;tout droit&quot;, elle est obligée de suivre la grille de pixels, c'est &quot;l'effet escalier&quot; :    Ce qui, vu de loin, donne un aspect rugueux :    L'anti-aliasing consiste à &quot;flouter&quot; l'objet pour que, vu de loin, il apparaisse plus lisse :    ","version":"Next","tagName":"h2"},{"title":"Les différentes techniques d'anti-aliasing​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#les-différentes-techniques-danti-aliasing","content":"     ","version":"Next","tagName":"h2"},{"title":"SSAA (Super-Sampling Anti-Aliasing)​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#ssaa-super-sampling-anti-aliasing","content":" ➕ Excellente qualité quand on monte le nombre de samples (e.g. 64) ➖ Très coûteux en performance ➖ Très mauvais ratio qualité / performance  La plus vieille technique d'anti-aliasing, elle est très simple : elle revient à dessiner sur une image N fois plus grande (N = nombre de samples), puis à la downscale à la taille désirée. Ca marche très bien, mais ça prend beaucoup de temps.  ","version":"Next","tagName":"h3"},{"title":"MSAA (Multi-Sampling Anti-Aliasing) et​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#msaa-multi-sampling-anti-aliasing-et","content":" Similaire à SSAA, en un peu moins quali et un peu meilleur en performance, mais reste très coûteux et a un mauvais ratio qualité / performance.  ","version":"Next","tagName":"h3"},{"title":"FXAA (Fast Approximate Anti-Aliasing)​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#fxaa-fast-approximate-anti-aliasing","content":" Une technique de post-processing, qui détecte les contours des objets et les floute. Relativement efficace et quali, mais a été remplacée par SMAA, qui fait la même chose en mieux.  ","version":"Next","tagName":"h3"},{"title":"SMAA (Subpixel Morphological Anti-Aliasing)​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#smaa-subpixel-morphological-anti-aliasing","content":" Technique récente, très quali, comme FXAA en plus quali, mais un peu plus coûteux.  ","version":"Next","tagName":"h3"},{"title":"TXAA (Temporal Anti-Aliasing)​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#txaa-temporal-anti-aliasing","content":" Anti-aliasing pour les objets qui bougent.  ","version":"Next","tagName":"h3"},{"title":"TAA (Temporal Anti-Aliasing)​","type":1,"pageTitle":"Anti-Aliasing","url":"/Rendering/Deep dive/Anti-Aliasing#taa-temporal-anti-aliasing","content":" À ne pas confondre avec TXAA! Les deux techniques n'ont rien à voir. TXAA gère les objets en mouvement, là où TAA réutilise les frames précédentes pour améliorer l'anti-aliasing.  Réutilise les informations des frames précédentes pour faire du &quot;multisampling&quot;. Elle est donc quasiment aussi quali que SSAA, mais sans l'énorme coût en performance. Technique très quali, très bon rapport qualité / performance, et sur laquelle de nombreux autre effets se basent. Elle a une place importante dans le pipe de rendu de tous les moteurs modernes. Rarement exposé dans les settings graphiques, car on ne peut pas le désactiver, puisque plein d'autres effets en dépendent. ","version":"Next","tagName":"h3"},{"title":"Graphics Settings","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/Graphics Settings","content":"","keywords":"","version":"Next"},{"title":"Activité​","type":1,"pageTitle":"Graphics Settings","url":"/Rendering/Deep dive/Graphics Settings#activité","content":" Allez dans votre jeu vidéo préféré, ou dans Unreal, ou Blender (utilisez EEVEE comme moteur de rendu, car c'est un moteur en rasterization, donc plus proche des moteurs de rendu de jeu vidéo).  ","version":"Next","tagName":"h2"},{"title":"Explications​","type":1,"pageTitle":"Graphics Settings","url":"/Rendering/Deep dive/Graphics Settings#explications","content":" Je recommande ces deux vidéos :     ","version":"Next","tagName":"h2"},{"title":"LOD - Level of Detail","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/LOD - Level of Detail","content":"LOD - Level of Detail","keywords":"","version":"Next"},{"title":"Tesselation","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/Tesselation","content":"Tesselation Rajoute de la géométrie automatiquement sur les meshs proches, pour qu'ils aient d'avantage de détails. Permet essentiellement d'ensuite appliquer une displacement map. https://youtu.be/1UcBwsQTwwI?t=348","keywords":"","version":"Next"},{"title":"Normal maps","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/Normal maps","content":"","keywords":"","version":"Next"},{"title":"Qu'est-ce qu'une normal map ?​","type":1,"pageTitle":"Normal maps","url":"/Rendering/Deep dive/Normal maps#quest-ce-quune-normal-map-","content":" Une normal map permet de rajouter des détails et une impression de relief sur un mesh avec relativement peu de triangles. Cela permet d'économiser beaucoup de performances, et est donc indispensable pour les assets d'un jeu vidéo !  Les normales d'un modèle sont les informations qui indiquent dans quelle direction la surface est orientée, et elles permettent de calculer les effets de lumière. Ce sont les flèches grises sur ce schéma :  Par défaut, elles sont calculées en fonction des triangles, et donc plus il y a de triangles plus les normales sont variées et rajoutent du relief. Mais il est possible de sauvegarder ces normales dans une texture, afin que chaque pixel du triangle ait une normale différente. On peut ainsi garder les informations des normales, même en ayant moins de triangles.  La seule chose qu'on perd avec une normal map, c'est le volume réel : en mettant votre vue rasante avec l'objet, vous verrez qu'il est plat et l'illusion est cassée.  ","version":"Next","tagName":"h2"},{"title":"TP : créer une normal map​","type":1,"pageTitle":"Normal maps","url":"/Rendering/Deep dive/Normal maps#tp--créer-une-normal-map","content":" Pour ce TP nous allons utiliser Blender. Si vous ne l'avez pas déjà, vous pouvez le télécharger ici. NB : j'utilise la version 4.1 de Blender (la dernière en date), mais vous devriez pouvoir suivre avec n'importe quelle version de Blender.Téléchargez le modèle qui nous servira d'exemple et importez-le dans Blender. (Si vous avez Blender 4.1, il vous suffit de drag'n drop le fichier. Et sinon, allez dans le menu File &gt; Import &gt; Wavefront (.obj))En affichant le wireframe du mesh, vous verrez qu'il contient ÉNORMÉMENT de triangles. C'est très bien pour du cinéma où on veut la qualité maximale, mais pour du jeu vidéo le coût en performance sera beaucoup trop élevé, pour un gain de qualité pas si important. On a moyen de beaucoup alléger le mesh, tout en gardant une grosse partie des détails, grâce à une normal map.Vous pouvez afficher le nombre de triangles en activant les statistiquesPour créer une normal map, suivez le tuto ci-dessous. Lors du decimate (réduction du nombre de triangles), vous pouvez mettre le ratio à 0.001 (Vous pouvez aussi expérimenter avec d'autres ratios, et voir jusqu'où vous pouvez aller tout en préservant la qualité du rendu. Vous pouvez même créer un simple cylindre, le rendu sera pas mal du tout !)   ","version":"Next","tagName":"h2"},{"title":"VSync","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/VSync","content":"VSync Evite le tearing, en limitant le framerate. Risque : si le jeu tourne à un poil moins de 60fps, genre 59fps, alors il va automatiquement devoir se limiter à 30fps pour rester sync avec l'écran. GSync : tech dans certains écrans, qui les fait se refresh dès qu'ils recoivent une nouvelle image de la carte graphique. Comme ça c'est l'écran qui s'adapte à la CG et plus l'inverse, donc on peut rendre les frames aussi vite qu'on veut / peut. https://youtu.be/1UcBwsQTwwI?t=249","keywords":"","version":"Next"},{"title":"Consignes du rendu","type":0,"sectionRef":"#","url":"/Rendering/M1 GP/Consignes du rendu","content":"Consignes du rendu Le rendu de vos TPs se fera via GitHub / GitLab, en me mettant un lien sur ce Google Sheet. ATTENTION à bien vérifier que votre repo est en public ! Il y aura deux notes, une au milieu du trimestre (après la séance du 21 février) et une à la fin (après la séance du 13 mars). Il est donc important d'avancer régulièrement, car vous serez évalué.es au milieu du trimestre sur là où vous en êtes. Il y a quelques sections qui sont marquées Bonus dans les TPs, vous n'êtes pas obligé.es de les faire, mais si vous en avez le temps et l'envie n'hésitez pas, ça sera valorisé dans la notation. NB : Votre implication et votre participation en classe influencerons aussi (légèrement) votre note. IMPORTANT Vous devriez avoir le temps de faire les TPs avec le temps qu'on a en classe, ne vous sentez pas obligé.es d'avancer à la maison. (Et si les TPs s'avèrent être trop longs pour tout le monde, faites-le moi savoir et j'adapterai mes exigences, pas de panique).","keywords":"","version":"Next"},{"title":"Intro","type":0,"sectionRef":"#","url":"/Rendering/M1 GP/Intro","content":"Intro Dans ce cours nous allons écrire notre petit moteur de rendu from scratch, en C++ et en utilisant l'API OpenGL. Le but est que vous découvriez les différents concepts qui sont au cœur d'un moteur de rendu 3D, et que vous compreniez mieux ce qu'il se passe derrière vos outils préférés (Unreal, Unity, Blender, etc.). Nous allons parler de : MeshVertex BufferIndex BufferVertex ShaderFragment ShaderVariables uniformesCaméraMatrice de vueMatrice de projectionMatrice modèleDepth BufferTexturesModèles d'éclairage (Blinn–Phong, PBR)Post-processingRender TargetOmbres (Shadow Maps) À chaque séance je ferai un petit point de cours (30 min max) pour vous expliquer un concept en détail, puis vous serez en autonomie pour avancer sur les TPs. N'hésitez surtout pas à me poser plein de questions, et à m'appeler dès que vous êtes bloqué.es plus de 5-10 minutes sur quelque chose. Vous serez évalué.es sur ces TPs qu'il faudra me rendre. N'hésitez pas à me faire des retours sur tout problème qu'il pourrait y avoir ! Ce qui m'importe c'est que ce cours soit intéressant et enrichissant pour vous, donc je peux le faire évoluer et le changer si il ne vous convient pas !","keywords":"","version":"Next"},{"title":"Shadow maps & Ambient Occlusion","type":0,"sectionRef":"#","url":"/Rendering/Deep dive/Shadow maps & Ambient Occlusion","content":"","keywords":"","version":"Next"},{"title":"Shadow Map​","type":1,"pageTitle":"Shadow maps & Ambient Occlusion","url":"/Rendering/Deep dive/Shadow maps & Ambient Occlusion#shadow-map","content":" ","version":"Next","tagName":"h2"},{"title":"Qu'est-ce qu'une shadow map ?​","type":1,"pageTitle":"Shadow maps & Ambient Occlusion","url":"/Rendering/Deep dive/Shadow maps & Ambient Occlusion#quest-ce-quune-shadow-map-","content":" L'idée est de prendre un screenshot depuis le point de vue de la caméra, pour savoir ce qu'elle peut et ne peut pas &quot;voir&quot;, et donc en déduire ce qui est dans la lumière ou dans l'ombre.  AvecSans  Pour tester les shadow maps et l'occlusion ambiante, vous pouvez aller sur ce site, télécharger IMACUBES-Windows.zip, le dézipper, et lancer IMACUBES.exe.    Et vous pouvez activer / désactiver les ombres et changer leurs paramètres :  ","version":"Next","tagName":"h3"},{"title":"Limites​","type":1,"pageTitle":"Shadow maps & Ambient Occlusion","url":"/Rendering/Deep dive/Shadow maps & Ambient Occlusion#limites","content":" Sur cette vidéo on voit une des limitations : seuls les objets proches ont des ombres, et elles apparaissent subitement quand on se rapproche des objets.  ","version":"Next","tagName":"h3"},{"title":"Ambient Occlusion​","type":1,"pageTitle":"Shadow maps & Ambient Occlusion","url":"/Rendering/Deep dive/Shadow maps & Ambient Occlusion#ambient-occlusion","content":" Les shadow maps à elles seules ne suffisent pas à capturer toutes les ombres possibles de manière photoréaliste. C'est pourquoi d'autres techniques viennent les complémenter. L'ambient occlusion s'occupe d'obscurcir les petits recoins.  AvecSans ","version":"Next","tagName":"h2"},{"title":"Intro","type":0,"sectionRef":"#","url":"/Rendering/M2 GP/Intro","content":"Intro TP1 : shadow maps, en implem une, bien comprendre ses paramètres (et faire le parallèle avec les ombres dans Unity et Unreal, apprendre comment bien les configurer, ce que font chaques paramètres)","keywords":"","version":"Next"},{"title":"Ressources","type":0,"sectionRef":"#","url":"/Rendering/Ressources","content":"","keywords":"","version":"Next"},{"title":"OpenGL​","type":1,"pageTitle":"Ressources","url":"/Rendering/Ressources#opengl","content":" Acerola : super chaîne Youtube qui vulgarise des concepts / techniques de rendu, et décortique le rendu de certains jeux vidéos Vidéo : Les éléments de base du rendu 3D vulgarisés Vidéos : Cours avancé : Plus théorique, mais très complet et très bien expliqué Vidéos : Cours avancé : Comment écrire son propre moteur de rendu en C++ et OpenGL  ","version":"Next","tagName":"h2"},{"title":"Math​","type":1,"pageTitle":"Ressources","url":"/Rendering/Ressources#math","content":" Cours de maths pour le jeu vidéo, sur Youtube Game Math Book (dispo en ligne) ","version":"Next","tagName":"h2"},{"title":"Cours 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 1/Cours 1","content":"","keywords":"","version":"Next"},{"title":"Rappels​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 1/Cours 1#rappels","content":" Un shader est un programme qui s'exécute sur le GPU (contrairement aux programmes dont vous avez l'habitude, qui tournent sur le CPU).  La force du GPU est d'être très parallélisé : là où un CPU aura quelques dizaines de threads, un GPU en a des milliers ! Il peut donc faire énormément d'opérations en parallèle, par exemple traiter des milliers de pixels en même temps, ou des milliers de sommets d'un mesh.  Il existe différents types de shaders :  le vertex shader process des sommets : il reçoit toutes les infos d'un sommet (position, normale, etc.) et les modifie (par exemple positionne le modèle au bon endroit dans le monde, applique la caméra, etc.)le fragment shader process des pixels : il reçoit des infos du vertex shader (qui s'exécute avant), et doit retourner une couleur pour le pixel en question, et appliquant l'éclairage, nos effets custom, etc.  Dans le Material Editor de Unreal ces deux shaders sont présentés dans un seul graphe, mais il est bon de toujours se rappeler que ce sont deux étapes séparées : d'abord le vertex, puis le fragment.  le compute shader est en dehors de la pipeline classique du rendu 3D. C'est un shader plus récent, et plus générique. Il prend des tableaux quelconques, et les modifie comme il veut. C'est de plus en plus utilisés pour accélérer certains systèmes, par exemple les simulations de cloth, ou les particules.  Il existe aussi d'autre types beaucoup plus niches, comme le geometry shader et le tesselation shader, dont nous ne parlerons pas.  ","version":"Next","tagName":"h2"},{"title":"Fonctions utiles​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 1/Cours 1#fonctions-utiles","content":" Add: translation Multiply: scale Step / SmoothStep: seuils Frac / Fmod: répétitions Sine (sinus): oscillations Lerp / InvLerp / RemapValueRange: remapper des valeurs ","version":"Next","tagName":"h2"},{"title":"Partie 1 - Setup","type":0,"sectionRef":"#","url":"/Rendering/M1 GP/Partie 1 - Setup","content":"","keywords":"","version":"Next"},{"title":"Installer Git​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-git","content":" Téléchargez 64-bit Git for Windows Setup depuis cette page. Vous pouvez laisser toutes les options par défaut et installer.  Git est un outil de versioning qui vous permettra de sauvegarder votre code régulièrement, et de me le partager.  ","version":"Next","tagName":"h2"},{"title":"Installer un compilateur C++​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-un-compilateur-c","content":" Téléchargez Build Tools for Visual Studio 2022 depuis cette page, lancez l'exe, puis cochez Desktop development with C++ et installez.    Un compilateur va transformer votre code en un programme exécutable. Vous ne l'utiliserez pas directement, mais il est nécessaire pour que tout build correctement.  ","version":"Next","tagName":"h2"},{"title":"Installer CMake​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-cmake","content":" Téléchargez Windows x64 Installer depuis cette page. IMPORTANT : pendant l'installation, cochez bien Add CMake to the PATH environment variable.    CMake est l'outil de build le plus répandu pour C++. Il explique au compilateur comment compiler votre projet. Vous ne l'utiliserez pas directement, mais il est nécessaire pour que tout build correctement.  ","version":"Next","tagName":"h2"},{"title":"Installer VSCode​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-vscode","content":" Téléchargez-le depuis cette page.  VSCode est l'IDE (&quot;éditeur de texte&quot;) que je vous conseille d'utiliser pour écrire votre code C++. (Vous pouvez utiliser un autre IDE si vous voulez, mais ce cours n'expliquera pas comment le setup pour faire compiler votre projet).  ","version":"Next","tagName":"h2"},{"title":"Installer les extensions VSCode pour C++​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-les-extensions-vscode-pour-c","content":" cpptools-extension-pack: Les extensions C++ de base, obligatoires pour compiler votre projet.webgl-glsl-editor: L'extension GLSL, pour avoir de l'autocomplétion quand vous écrirez des shaders.BuildOutputColorizer: Pour que les messages d'erreur apparaissent en rouge dans la console, très pratique.code-spell-checker: Un correcteur d'orthographe. Marche pour l'anglais et le français. (Pour le configurer pour le français, installez en plus code-spell-checker-french et suivez les instructions qui sont sur la page de téléchargement).vscode-great-icons: Des icônes pour plein de types de fichier (.cpp, .hpp, CMakeLists.txt, etc.).cmake-language-support-vscode: Autocomplétion pour CMake.  ","version":"Next","tagName":"h2"},{"title":"Installer RenderDoc​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#installer-renderdoc","content":" Téléchargez-le depuis cette page.  RenderDoc est un débugueur pour GPU. Il permet de voir tout ce qu'il s'est passé dans la carte graphique pendant le rendu d'une frame donnée. Il permet de voir l'état de vos meshs, les différentes étapes de la construction de l'image finale, et plein d'autres choses encore ! Nous allons beaucoup nous en servir au fil des TPs, à la fois pour débuguer notre code, et aussi juste pour inspecter et mieux comprendre ce qu'il se passe dans la carte graphique.  ","version":"Next","tagName":"h2"},{"title":"Télécharger la base de code​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#télécharger-la-base-de-code","content":" Vous êtes maintenant prêt.es à coder ! Nous allons partir de ce template de code qui inclue toutes les libraires nécessaires. Vous pouvez directement créer un repository GitHub à partir de celui-ci en faisant Use this template :    (Et sinon vous pouvez aussi juste télécharger le code normalement, et créer un repo de votre côté).  ","version":"Next","tagName":"h2"},{"title":"Compiler​","type":1,"pageTitle":"Partie 1 - Setup","url":"/Rendering/M1 GP/Partie 1 - Setup#compiler","content":" Pour lancer votre projet, il vous suffit maintenant d'appuyer sur le petit insecte depuis VSCode. La première fois, il va vous demander quel compilateur vous voulez utiliser : il faudra probablement faire [Scan for kits], puis sélectionnez le compilateur qui mentionne amd64.   ","version":"Next","tagName":"h2"},{"title":"Cours 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2","content":"","keywords":"","version":"Next"},{"title":"Noises et autres textures​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#noises-et-autres-textures","content":" ","version":"Next","tagName":"h2"},{"title":"White Noise​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#white-noise","content":"   ","version":"Next","tagName":"h3"},{"title":"Perlin / Simplex Noise​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#perlin--simplex-noise","content":"   ","version":"Next","tagName":"h3"},{"title":"Fractal Noise​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#fractal-noise","content":"   ","version":"Next","tagName":"h3"},{"title":"Voronoi (distance au centre de la cellule)​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#voronoi-distance-au-centre-de-la-cellule","content":"   ","version":"Next","tagName":"h3"},{"title":"Voronoi (ID de la cellule)​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 1/Cours 2#voronoi-id-de-la-cellule","content":"  ","version":"Next","tagName":"h3"},{"title":"Exos 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1","content":"","keywords":"","version":"Next"},{"title":"Refaire la bande qui scanne​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#refaire-la-bande-qui-scanne","content":"   ","version":"Next","tagName":"h2"},{"title":"Plusieurs hachures​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#plusieurs-hachures","content":"   ","version":"Next","tagName":"h2"},{"title":"Aller dans l'autre sens​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#aller-dans-lautre-sens","content":"   ","version":"Next","tagName":"h2"},{"title":"Se rejoindre au milieu​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#se-rejoindre-au-milieu","content":"   ","version":"Next","tagName":"h2"},{"title":"En diagonale​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#en-diagonale","content":"   ","version":"Next","tagName":"h2"},{"title":"En cercle​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 1/Exos 1#en-cercle","content":"  ","version":"Next","tagName":"h2"},{"title":"Partie 3 - Du vrai rendu 3D","type":0,"sectionRef":"#","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D","content":"","keywords":"","version":"Next"},{"title":"Charger un mesh depuis un fichier​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#charger-un-mesh-depuis-un-fichier","content":" Pour commencer, nous allons enfin utiliser de vrais modèles 3D. Pour cela, nous allons utiliser la librairie tinyobjloader qui lit le format de fichier .obj (un format simple de modèle 3D, qui est essentiellement une longue liste de sommets avec positions, UVs, normales, etc.). La librairie est déjà inclue par opengl-framework, vous n'avez rien à faire de ce côté là.  Il va nous falloir créer un tableau de floats qu'on va remplir avec les positions, UVs et normales du mesh, et nous allons ensuite utiliser ce tableau comme data pour créer un gl::Mesh :  auto load_mesh(std::filesystem::path const&amp; path) -&gt; gl::Mesh { // On lit le fichier avec tinyobj auto reader = tinyobj::ObjReader{}; reader.ParseFromFile(gl::make_absolute_path(path).string(), {}); if (!reader.Error().empty()) throw std::runtime_error(&quot;Failed to read 3D model:\\n&quot; + reader.Error()); if (!reader.Warning().empty()) std::cout &lt;&lt; &quot;Warning while reading 3D model:\\n&quot; + reader.Warning(); // On met tous les attributs dans un tableau auto vertices = std::vector&lt;float&gt;{}; for (auto const&amp; shape : reader.GetShapes()) { for (auto const&amp; idx : shape.mesh.indices) { // Position vertices.push_back(reader.GetAttrib().vertices[3 * idx.vertex_index + 0]); vertices.push_back(reader.GetAttrib().vertices[3 * idx.vertex_index + 1]); vertices.push_back(reader.GetAttrib().vertices[3 * idx.vertex_index + 2]); // UV vertices.push_back(reader.GetAttrib().texcoords[2 * idx.texcoord_index + 0]); vertices.push_back(reader.GetAttrib().texcoords[2 * idx.texcoord_index + 1]); // Normale vertices.push_back(reader.GetAttrib().normals[3 * idx.normal_index + 0]); vertices.push_back(reader.GetAttrib().normals[3 * idx.normal_index + 1]); vertices.push_back(reader.GetAttrib().normals[3 * idx.normal_index + 2]); }; } // TODO créer et return un gl::mesh, qui utilisera le tableau `vertices` en tant que `data` pour son vertex buffer. // Attention, il faudra bien spécifier le layout pour qu'il corresponde à l'ordre des attributs dans le tableau `vertices`. }   Complétez la fonction ci-dessus pour construire un mesh à partir du tableau de vertices obtenu grâce à tinyobj. Il vous suffit ensuite de remplacer votre mesh de cube par un mesh loadé avec cette fonction, et le tour est joué ! Pour vos tests, vous pouvez utiliser ce modèle 3D et le mettre dans votre dossier res.    astuce Si votre modèle est penché sur le côté au début, c'est &quot;normal&quot;. Il n'y a pas de convention universelle pour l'axe qui pointe vers le haut : certaines utilisent Y, et d'autres Z. Il faudra donc légèrement modifier les vertexs du mesh au moment du loading afin de faire pointer le bon axe vers le haut.  astuce N'essayez pas de faire un index buffer, les sommets tels qu'ils sont donnés par tinyobjloader ont un index buffer différent pour chaque attribut, ce qui n'est pas supporté par OpenGL. Il faudrait plutôt inspecter tous les sommets pour détecter ceux qui ont les mêmes positions et UVs et normales et recréer notre propre index buffer à partir de ça.  Note Il y a aussi un fichier .mtl dans le modèle que je vous ai fourni. Il décrit le matériau de l'objet, nous en parlerons plus tard.  ","version":"Next","tagName":"h2"},{"title":"Premier modèle d'éclairage et Normales​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#premier-modèle-déclairage-et-normales","content":" ","version":"Next","tagName":"h2"},{"title":"Normales​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#normales","content":" Nous allons maintenant commencer à éclairer plus ou moins nos objets en fonction des différentes lumières présentes dans la scène. Tout va se passer dans le fragment shader, qui va être responsable de faire ces calculs pour chaque pixel, et d'atténuer plus ou moins la couleur de base du pixel (appelée albedo) en fonction de la quantité de lumière reçue.  Pour faire ces calculs de lumière, nous avons besoin de connaître l'orientation de notre surface (pour savoir à quel point elle est face à la lumière) et c'est à ça que servent les normales :    La normale est le vecteur perpendiculaire à la surface ; on pourrait le calculer à la volée en connaissant les trois sommets du triangle, mais il est le plus souvent précalculé et stocké dans le vertex buffer. Par exemple dans le .obj que nous utilisons, les normales sont déjà stockées dans le fichier, en plus des positions et des UVs. Rajoutez les normales dans votre vertex buffer, et dans un premier temps pour vérifier qu'elles sont bonnes, affichez les en tant que couleur de l'objet :    On peut interpréter les couleurs : RGB = XYZ : en bleu ce sont les normales qui pointent vers Z, i.e. vers le haut. En rouge et vert sont celles qui pointent vers X et Y.  ","version":"Next","tagName":"h3"},{"title":"Lumière directionnelle​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#lumière-directionnelle","content":" Maintenant il nous faut décrire notre lumière. Nous allons commencer par le type le plus simple, les lumières directionnelles. C'est une lumière qui éclaire dans une seule direction ; c'est typiquement le cas du soleil (ou toute autre source très éloignée), dont tous les rayons nous arrivent (quasiment) parallèles :    Pour la décrire il nous suffit de donner sa direction, qui sera un vec3 normalisé (i.e. de longueur 1). On peut soit la mettre en constante dans le fragment shader const vec3 light_direction = normalize(vec3(0.2, 0.3, -1.));, soit la passer en uniform uniform vec3 light_direction; (ce qui est l'option la plus courante, car on veut pouvoir changer la direction de notre lumière sans avoir à recréer un nouveau shader à chaque fois). Une fois que vous avez cette direction, pour mesurer à quel point notre triangle fait face à la lumière (et donc à quel point il est éclairé) nous allons utiliser le produit scalaire (dot(v1, v2) en glsl). Le produit scalaire entre deux vecteurs parfaitement alignés vaut 1 (si les vecteurs sont normalisés), il vaut -1 si les vecteurs sont parfaitement opposés, et il vaut 0 si les vecteurs sont orthogonaux. On peut ensuite multiplier la couleur de notre pixel par le résultat de ce produit scalaire (avec un signe moins), ce qui va nous donner un premier modèle d'éclairage simpliste : Quand la normale et la direction de la lumière sont parfaitement opposées c'est l'éclairage maximal, et on va multiplier par un produit scalaire qui vaut 1, donc conserver toute la couleur du pixel. À l'inverse quand les vecteurs sont perpendiculaires aucune lumière n'arrive sur notre surface, et ça correspond bien au fait de multiplier par un produit scalaire qui vaut 0 dans ce cas.    astuce Pour nos produits scalaires on a besoin que les deux vecteurs soient normalisés, donc pensez bien à toujours normaliser vos normales ! Et attention, il y a des petits pièges : par exemple si vos normales sont normalisées dans le vertex shader, et qu'ensuite vous les passez au fragment shader via une variable in / out, elles ne seront plus normalisées, car l'interpolation entre les sommets se produit, et la moyenne de vecteurs normalisés n'est pas nécessairement normalisée.  ","version":"Next","tagName":"h3"},{"title":"Lumière ambiante​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#lumière-ambiante","content":" Un premier défaut qu'on peut tout de suite voir c'est que certaines parties du bateau sont complètement noires, car elles sont à l'opposé de la lumière. Ce n'est pas très réaliste, car dans la vraie vie même si on n'est pas directement exposé à la lumière celle-ci rebondit sur les objets environnants et vient éclairer un peu partout. C'est ce qui s'appelle de l'illumination globale. Il y a des techniques pour simuler ça dans les moteurs de jeu, mais ça devient assez complexe. Pour faire simple on peut juste rajouter un terme de lumière ambiante qui va faire une approximation de cette illumination globale. Il suffit d'additionner un petit quelque chose (0.3 par exemple) à notre produit scalaire, pour l'empêcher de tomber jusqu'à 0 (et aussi, si le produit scalaire devient négatif, il faut le limiter à 0, afin que notre +0.3 ait de l'effet dans toutes les zones noires, et pas seulement dans celles où le produit scalaire est &gt; -0.3).    ","version":"Next","tagName":"h3"},{"title":"Lumière ponctuelle​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#lumière-ponctuelle","content":" Un autre type de lumière assez simple est la lumière ponctuelle. Elle est décrite par une position (vec3) et illumine de manière égale dans toutes les directions autour d'elle. C'est typiquement le cas des ampoules et autres petites sources de lumière.    Le calcul d'éclairage est le même que pour une lumière directionnelle. Mais il nous faut d'abord calculer la direction de la lumière, qui cette fois-ci dépend de la position du pixel concerné. Il faut donc récupérer la position dans le fragment shader, puis calculer le vecteur entre la position du pixel et la position de la lumière. De plus, l'intensité d'une lumière ponctuelle décroît à mesure qu'on s'en éloigne, donc il faut diviser le tout par la distance au carré1.    astuce Pour contrôler l'influence des différentes lumières, vous pouvez leur rajouter une intensité (a.k.a. un nombre que vous allez multiplier au résultat du produit scalaire) afin d'augmenter ou diminuer l'effet d'une lumière.  ","version":"Next","tagName":"h3"},{"title":"Normal Matrix et Model Matrix​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#normal-matrix-et-model-matrix","content":" Si on se met à appliquer une matrice modèle à notre objet, par exemple pour le faire tourner, on se rend compte d'un petit problème :    L'éclairage ne prend pas en compte la transformation de notre objet ! C'est comme si il ne tournait pas aux yeux de la lumière ! C'est parce que les positions et normales qu'on récupère depuis le vertex buffer sont exprimées en Object Space, alors que la position / direction de notre lumière est en World Space. Pour pouvoir faire nos calculs il faut que tout soit exprimé dans le même repère ! Pour convertir les positions et normales de nos pixels depuis l'Object Space vers le World Space, il faut leur appliquer la Model Matrix. Et je vous donne ces deux fonctions pour appliquer correctement une matrice à une position et à une direction en prenant bien en compte les coordonnées homogènes :  vec3 apply_matrix_to_position(mat4 matrix, vec3 point) { vec4 tmp = matrix * vec4(point, 1.); return tmp.xyz / tmp.w; } vec3 apply_matrix_to_direction(mat4 matrix, vec3 direction) { vec4 tmp = matrix * vec4(direction, 0.); return normalize(tmp.xyz); }     astuce Pour éviter toute confusion à l'avenir, je vous conseille de toujours ajouter un petit suffixe à vos variables de position et direction, pour bien indiquer dans quel espace elles sont exprimées : par exemple _os pour object space, _ws pour world space, _vs pour view space, etc. layout(location = 0) in vec3 in_position_os; uniform mat4 model_matrix; out vec3 position_ws; // ... void main() { // ... position_ws = apply_matrix_to_position(model_matrix, in_position_os); // ... }   De plus, ce n'est pas tout à fait la Model Matrix qu'il faut appliquer aux normales, mais la Normal Matrix, qui est l'inverse de la transposée de la Model Matrix (glm::inverse(glm::transpose(model_matrix))). Ça revient au même dans la plupart des cas, mais si jamais il y a une scale non-uniforme dans votre model matrix, alors ça déformerait les normales et il faut passer par la Normal Matrix pour corriger ça.   Normales incorrectement transformées par la Model Matrix  Si vous voulez savoir d'où vient la normal matrix et pourquoi c'est l'inverse de la transposée, je vous recommande ce petit article.  ","version":"Next","tagName":"h3"},{"title":"Lumières colorées​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#lumières-colorées","content":" On peut aussi associer une couleur à chaque lumière, simplement en multipliant sa contribution par la couleur désirée (vec3) :    ","version":"Next","tagName":"h3"},{"title":"À vous de jouer !​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#à-vous-de-jouer-","content":" Voici différents effets &quot;avancés&quot; que vous pouvez implémenter maintenant que vous connaissez les bases du rendu 3D !  ","version":"Next","tagName":"h2"},{"title":"Normal maps​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#normal-maps","content":" Les normal maps permettent d'ajouter du détail à un mesh en modifiant la normale pour chaque pixel du triangle. Voici par exemple un tuto que vous pouvez suivre pour implémenter des normal maps. Et le même en version vidéo.  ","version":"Next","tagName":"h3"},{"title":"Ombres​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#ombres","content":" Les shadow maps permettent de produire des ombres pour les lumières directionnelles. C'est le point de départ de toute une famille de technique plus raffinées, comme les omnidirectional shadow maps qui gèrent les lumières ponctuelles, et les cascaded shadow maps qui améliorent la qualité des ombres. Voici par exemple un tuto que vous pouvez suivre pour implémenter une shadow map basique. (Et vous verrez, c'est déjà un peu costaud !) Et le même tuto en version vidéo.  ","version":"Next","tagName":"h3"},{"title":"Effet see-through​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#effet-see-through","content":" Pour faire cette effet où on voit une silhouette des objets à travers les murs, il vous suffit, une fois toute la scène rendue normalement, de redessiner une deuxième fois l'objet sur lequel vous voulez appliquer l'effet, avec ce setup particulier :  Appliquer un shader spécial qui va donner le visuel de la silhouette, par exemple retourner une couleur unie pour faire simpleChanger le depth test, pour que l'objet ne s'affiche que si il est derrière d'autres objets. Je vous réfère à la documentation de glDepthFunc pour voir comment faire ça.    ","version":"Next","tagName":"h3"},{"title":"Matériaux et modèles d'éclairages plus avancés​","type":1,"pageTitle":"Partie 3 - Du vrai rendu 3D","url":"/Rendering/M1 GP/Partie 3 - Du vrai rendu 3D#matériaux-et-modèles-déclairages-plus-avancés","content":" Je vous réfère à ces tutos :  Basic Lighting, il y a un peu de redite de ce qu'on a vu, mais aussi des nouveautésMaterialsBlinn-PhongPBR part 1PBR part 2  Footnotes​ C'est dû au principe physique de conservation d'énergie : à un instant t la source émet une certaine quantité d'énergie, et cette énergie s'éloigne de la source, formant une sphère de plus en plus grande. Comme la même quantité d'énergie doit être présente sur une sphère de plus en plus grande, l'énergie en un point donné doit décroître proportionnellement à la surface de la sphère, soit R2R^2R2. ↩ ","version":"Next","tagName":"h3"},{"title":"Exos 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2","content":"","keywords":"","version":"Next"},{"title":"Suivre un chemin quelconque​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2#suivre-un-chemin-quelconque","content":"   ","version":"Next","tagName":"h2"},{"title":"Cases aléatoires​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2#cases-aléatoires","content":"   ","version":"Next","tagName":"h2"},{"title":"Cases qui s'allument progressivement​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2#cases-qui-sallument-progressivement","content":"   ","version":"Next","tagName":"h2"},{"title":"Utiliser l'effet précédent pour faire une transition entre deux images​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2#utiliser-leffet-précédent-pour-faire-une-transition-entre-deux-images","content":"   ","version":"Next","tagName":"h2"},{"title":"Créez vos propres effets et transitions !​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 1/Exos 2#créez-vos-propres-effets-et-transitions-","content":" Par exemple :   ","version":"Next","tagName":"h2"},{"title":"Cours 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1","content":"","keywords":"","version":"Next"},{"title":"Paramètres PBR​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#paramètres-pbr","content":" ","version":"Next","tagName":"h2"},{"title":"Metallic​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#metallic","content":" 0 = non-métal\t1 = métal\t  Notez que, si on veut rester réaliste, cette valeur doit être soit à 0 soit à 1. Des valeurs intermédiaires comme 0.3 ne font pas sens physiquement (mais rien ne vous empêche de les utiliser quand même si ça produit l'effet que vous recherchez ! De nombreux styles ne sont pas physiquement réalistes et ce n'est pas un problème !)  ","version":"Next","tagName":"h3"},{"title":"Specular​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#specular","content":" Contrôle la taille de la tache spéculaire, a.k.a. le petit reflet très lumineux :    NB : ça n'a d'effet que pour les matériaux non-métalliques.  ","version":"Next","tagName":"h3"},{"title":"Roughness​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#roughness","content":" Plus la valeur est grande, plus les réflexions sont floutées :  Roughness 0\tRoughness 0.5\t  ","version":"Next","tagName":"h3"},{"title":"Emissive Color​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#emissive-color","content":" Couleur émise par l'objet, comme si c'était une source de lumière. ATTENTION, si vous voulez que votre objet glow, il faut passer une couleur avec des valeurs supérieures à 1. Le plus simple est de choisir une couleur puis la multiplier par une intensité :    ","version":"Next","tagName":"h3"},{"title":"Anisotropy​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 2/Cours 1#anisotropy","content":" Nécessite d'utiliser une tangent map dans la pin Tangent.  Donne une direction aux reflections. C'est typiquement un effet qu'on voit sur les métaux brossés :     ","version":"Next","tagName":"h3"},{"title":"Cours 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 2/Cours 2","content":"Cours 2 Une technique très pratique consiste à utiliser une texture pour passer un gradient de couleurs complexe au matériau. Vous pouvez générer cette texture dans l'outil de votre choix, puis l'importer dans Unreal : On peut ensuite utiliser n'importe quel pattern en noir et blanc en tant qu'UV de la texture contenant le gradient de couleur, pour appliquer ces couleurs au pattern : NB : la texture de gradient n'a besoin de faire qu'un seul pixel de haut. Quand à la largeur, c'est à vous de faire un compromis entre la qualité de la résolution et la taille de la texture.","keywords":"","version":"Next"},{"title":"Exos","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 2/Exos","content":"Exos Essayer de reproduire ces matériaux : Tester d'utiliser les patterns de hachure de la dernière fois pour contrôler différents paramètres. Par exemple :","keywords":"","version":"Next"},{"title":"Cours 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 3/Cours 1","content":"","keywords":"","version":"Next"},{"title":"Scene Color​","type":1,"pageTitle":"Cours 1","url":"/Rendering/Shaders Unreal/Jour 3/Cours 1#scene-color","content":" On peut utiliser la texture SceneColor pour obtenir divers effets de déformation. Cette texture contient un rendu de tous les objets opaques de la scène, et est accessible aux objets transparents.  Pour sampler cette texture on utilise ScreenPosition, et c'est en déformant cette position qu'on peut obtenir divers effets.  Attention, pensez à passer votre matériau en Translucent Unlit : ","version":"Next","tagName":"h2"},{"title":"Cours 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 3/Cours 2","content":"","keywords":"","version":"Next"},{"title":"Normales et Vertex Displacement​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 3/Cours 2#normales-et-vertex-displacement","content":" Pour déplacer les sommets, on va souvent vouloir le faire dans la direction de la normale, afin de s'éloigner de la surface :    En changeant ce par quoi on multiplie la normale, et en utilisant des valeurs qui changent dans l'espace et / ou le temps, avec du noise ou tout autre masque, on peut obtenir de nombreux effets.  ","version":"Next","tagName":"h2"},{"title":"Masque en fonction de la normale​","type":1,"pageTitle":"Cours 2","url":"/Rendering/Shaders Unreal/Jour 3/Cours 2#masque-en-fonction-de-la-normale","content":" On peut isoler les parties du mesh qui sont orientées dans une certaine direction en mesurant à quel point la normale est alignée avec cette direction, grâce au produit scalaire (DotProduct). Pour rappel, le produit scalaire entre deux vecteurs unitaires1 vaut 1 quand ils sont parfaitement alignés, 0 quand ils sont perpendiculaires, et -1 quand ils sont parfaitement opposés. En combinant ça avec une SmoothStep on peut récupérer un masque qui indique les zones où le produit scalaire est proche de 1.      Footnotes​ i.e. de longueur 1 (ce qui est le cas des Normales par défaut). Si les vecteurs ne sont pas unitaires le résultat du produit scalaire sera multiplié par les longueurs des deux vecteurs. ↩ ","version":"Next","tagName":"h2"},{"title":"Exos 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 3/Exos 1","content":"","keywords":"","version":"Next"},{"title":"Déformation avec du noise​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 3/Exos 1#déformation-avec-du-noise","content":"   NB : pour éviter une rupture abrupte sur les bords du mesh, vous pouvez utiliser un masque pour atténuer progressivement l'effet au fur et à mesure qu'on se rapproche des bords du mesh.  ","version":"Next","tagName":"h2"},{"title":"Lentille​","type":1,"pageTitle":"Exos 1","url":"/Rendering/Shaders Unreal/Jour 3/Exos 1#lentille","content":"  ","version":"Next","tagName":"h2"},{"title":"Exos 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 3/Exos 2","content":"","keywords":"","version":"Next"},{"title":"Pulsation​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 3/Exos 2#pulsation","content":"   ","version":"Next","tagName":"h2"},{"title":"Noise​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 3/Exos 2#noise","content":"   ","version":"Next","tagName":"h2"},{"title":"Limiter l'effet aux parties horizontales du mesh​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 3/Exos 2#limiter-leffet-aux-parties-horizontales-du-mesh","content":"   ","version":"Next","tagName":"h2"},{"title":"Onde de choc​","type":1,"pageTitle":"Exos 2","url":"/Rendering/Shaders Unreal/Jour 3/Exos 2#onde-de-choc","content":"  ","version":"Next","tagName":"h2"},{"title":"Cours","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/Jour 4/Cours","content":"","keywords":"","version":"Next"},{"title":"Paramètres exposés​","type":1,"pageTitle":"Cours","url":"/Rendering/Shaders Unreal/Jour 4/Cours#paramètres-exposés","content":" Il est possible d'exposer des paramètres pour qu'ils soient modifiables depuis l'éditeur et / ou depuis le code. Pour cela, au lieu d'utiliser un node Constant, ou va utiliser un node Parameter (ScalarParameter, VectorParameter, TextureParameter, etc.) (NB: il est aussi possible de convertir un node Constant existant en Parameter avec un clic-droit :    ","version":"Next","tagName":"h2"},{"title":"Customisation statique​","type":1,"pageTitle":"Cours","url":"/Rendering/Shaders Unreal/Jour 4/Cours#customisation-statique","content":" Si vous voulez créer une variante du matériau, qui utilise des valeurs différentes pour un ou plusieurs paramètres, et que ces valeurs ne vont pas changer à runtime, vous pouvez créer une Material Instance à partir de ce Material :    Puis sélectionner les paramètres qu'on veut changer dans cette instance :    Avoir des Material Instance permet de faire des variantes d'un matériau. C'est très performant car Unreal peut rendre tous les objets qui utilisent des Material Instance venant du même Material en une seule passe de rendu. C'est donc bien mieux que de faire différents Material qui ne diffèrent que par la valeur d'une ou deux constantes.  ","version":"Next","tagName":"h2"},{"title":"Customisation dynamique​","type":1,"pageTitle":"Cours","url":"/Rendering/Shaders Unreal/Jour 4/Cours#customisation-dynamique","content":" Si vous voulez pouvoir changer les paramètres à runtime, en fonction d'actions gameplay ou autre, alors il ne faut pas faire de Material Instance mais plutôt, directement depuis un script, créer un Dynamic Material Instance :    Puis vous pouvez quand vous voulez changer la valeur des paramètres du matériau :   ","version":"Next","tagName":"h2"},{"title":"RENDU 1","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/RENDU 1","content":"RENDU 1 Faire une animation de carte brillante pour indiquer la rareté de certaines cartes Carte Royale dans Yu-Gi-Oh! Master Duel, et pour comparaison, la même carte sans effet de rareté : Vous n'êtes pas obligé.es de produire un copier-coller de cet effet, mais je m'attend à quelque chose d'une complexité similaire (ou plus élevée ^^). Vous pouvez refaire exactement le même effet si vous le souhaitez. Si pertinent pour votre jeu, intégrez le au projet de Studio Time. Si vous êtes plusieurs à travailler sur le même projet en studio time, faites un effet par personne, ceci est un rendu individuel. Pour le rendu, déposez sur ce drive, dans un dossier à votre nom, un gif / vidéo de l'effet, ainsi qu'un fichier texte contenant le lien git (vers la bonne branche !) ET LE NOM DU MATÉRIAU afin que je puisse le retrouver dans le projet. (Attention à ce que le repo soit en public, ou alors invitez moi dessus (JulesFouchy)). Deadline : 13 décembre à 9h45","keywords":"","version":"Next"},{"title":"RENDU 2","type":0,"sectionRef":"#","url":"/Rendering/Shaders Unreal/RENDU 2","content":"RENDU 2 Reproduire cet effet Voici les points que vous devrez implémenter : Tremblement du modèleDisparition progressive (vous pouvez utiliser ObjectPositionWS pour obtenir le centre du modèle)Utiliser un noise pour donner des contours organiques à la disparitionGlow, sauf sur les parties qui s'apprêtent à disparaîtreActiver cet effet depuis le gameplay (quand on s'approche de l'objet, ou quand on lui tire dessus, ou ce que vous voulez). Puis enlever l'objet de la scène quand l'effet de disparition est terminé. (Cette vidéo peut vous aider) Pour le rendu, déposez sur ce drive, dans un dossier à votre nom, un gif / vidéo de l'effet, ainsi qu'un fichier texte contenant le lien git (vers la bonne branche !) ET LE NOM DU MATÉRIAU afin que je puisse le retrouver dans le projet. (Attention à ce que le repo soit en public, ou alors invitez moi dessus (JulesFouchy)). Deadline : Dimanche 22 décembre à 23h59","keywords":"","version":"Next"},{"title":"Partie 2 - Bases du rendu","type":0,"sectionRef":"#","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu","content":"","keywords":"","version":"Next"},{"title":"La structure de l'application​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#la-structure-de-lapplication","content":" Pour l'instant dans src/main.cpp vous avez ceci :  #include &quot;opengl-framework/opengl-framework.hpp&quot; // Inclue la librairie qui va nous servir à faire du rendu int main() { // Initialisation gl::init(&quot;TPs de Rendering&quot;); // On crée une fenêtre et on choisit son nom gl::maximize_window(); // On peut la maximiser si on veut while (gl::window_is_open()) { // Rendu à chaque frame } }   La boucle while est ce qu'on appelle la boucle de rendu ; chaque itération correspond à une frame, et il faudra y mettre le code dessinant ce qu'on veut pour cette frame.  En exécutant ce code, vous devriez avoir une fenêtre noire qui s'ouvre :  ","version":"Next","tagName":"h2"},{"title":"Couleur de fond​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#couleur-de-fond","content":" Pour commencer très simplement on peut choisir la couleur de fond, au début de la boucle de rendu :  glClearColor(0.f, 0.f, 1.f, 1.f); // Choisis la couleur à utiliser. Les paramètres sont R, G, B, A avec des valeurs qui vont de 0 à 1 glClear(GL_COLOR_BUFFER_BIT); // Exécute concrètement l'action d'appliquer sur tout l'écran la couleur choisie au-dessus     Note En plus de choisir la couleur, l'opération glClear(GL_COLOR_BUFFER_BIT) est très importante car elle reset l'image entre deux frames. Sans elle, les objets dessinés ne disparaissent pas d'une frame à l'autre (ce qui peut permettre des effets artistiques intéressants, mais est embêtant pour un rendu &quot;classique&quot;). Vous pourrez essayer ça un peu plus tard quand nous saurons dessiner des objets qui bougent.  ","version":"Next","tagName":"h2"},{"title":"OpenGL​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#opengl","content":" Nous utilisons l'API OpenGL pour communiquer avec la carte graphique. Il en existe d'autres, mais elles sont soit plus difficiles à apprendre (Vulkan, WebGPU), soit spécifiques à un OS (DirectX pour Windows, Metal pour Mac).  Si jamais vous cherchez de l'aide sur Internet ou ChatGPT, pensez à bien préciser OpenGL dans votre recherche. Et sinon, vous trouverez la documentation de toutes les fonctions OpenGL sur docs.gl.  Remarque Tout ce qui commence par gl:: (comme gl::init()) ne fait pas partie de l'API OpenGL de base, mais d'un wrapper que je vous fournis pour vous simplifier la vie. Par contre ce qui commence par gl (comme glClearColor) fait partie de l'API OpenGL officielle.  ","version":"Next","tagName":"h2"},{"title":"Mesh​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#mesh","content":" ","version":"Next","tagName":"h2"},{"title":"Vertex Buffer et Premier Triangle​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#vertex-buffer-et-premier-triangle","content":" Nous allons maintenant dessiner notre premier objet ! Pour décrire notre objet à la carte graphique, nous utilisons un mesh, c'est-à-dire une longue liste de triangles qui, mis bout-à-bout, dessinent notre forme en 3D :    Pendant l'initialisation nous pouvons créer un objet de type gl::Mesh :  auto const triangle_mesh = gl::Mesh{{ .vertex_buffers = {{ .layout = {gl::VertexAttribute::Position2D{0 /*Index de l'attribut dans le shader, on en reparle juste après*/}}, .data = { -1.f, -1.f, // Position2D du 1er sommet +1.f, -1.f, // Position2D du 2ème sommet 0.f, +1.f // Position2D du 3ème sommet }, }}, }};   que nous allons ensuite dessiner à chaque frame dans la boucle de rendu :  gl::bind_default_shader(); // On a besoin qu'un shader soit bind (i.e. &quot;actif&quot;) avant de draw(). On en reparle dans la section d'après. triangle_mesh.draw(); // C'est ce qu'on appelle un &quot;draw call&quot; : on envoie l'instruction à la carte graphique de dessiner notre mesh.     Il y a déjà plein de chose à dire ! Quand on crée un mesh, on lui passe un vertex buffer :  .vertex_buffers = {{ .layout = {gl::VertexAttribute::Position2D{0 /*Index de l'attribut dans le shader, on en reparle juste après*/}}, .data = { -1.f, -1.f, // Position2D du 1er sommet +1.f, -1.f, // Position2D du 2ème sommet 0.f, +1.f // Position2D du 3ème sommet }, }},   Un vertex buffer c'est le tableau qui contient toutes les données décrivant notre mesh : position des sommets des triangles, mais également plein d'autres données optionnelles : couleur, normale, coordonnée de texture (UV), etc. En fait c'est nous qui décidons quoi mettre dans ce buffer, puis que faire de ces données quand on code le vertex shader (que nous verrons plus tard). Pour faire du rendu classique on utilise généralement position + normales + UV. Mais on peut aussi imaginer d'autres usages plus créatifs et rajouter toutes les données dont on pourrait avoir besoin pour un effet précis.  Remarque Un usage original : on peut stocker la distance au tronc sur chaque vertex des branches et feuilles des arbres. Cette distance est ensuite utilisée pour calculer à quel point la branche peut ployer sous l'effet du vent. (La partie attachée au tronc ne doit pas bouger, et plus on s'en éloigne plus on peut bouger librement). Stocker cette distance dans le vertex buffer évite de la recalculer à chaque frame : c'est beaucoup plus optimisé. C'est utilisé dans God of War par exemple :  Décrire ce vertex buffer se passe en deux étapes : son layout et ses data. Le layout indique comment data est structuré : dans notre exemple on a juste des positions 2D qui s'enchaînent, mais ça pourrait aussi être des positions 3D, et on pourrait aussi avoir d'autres attributs dans le même tableau data, par exemple des coordonnées de texture, comme on verra plus tard. Pour chaque attribut du layout, il faut préciser son index (0 dans notre cas), une information qui nous servira plus tard pour récupérer l'attribut du côté du shader.  Vous remarquerez aussi que vertex_buffers est un tableau de vertex buffers ! Il est en effet possible d'avoir plusieurs vertex buffers, par exemple un qui stocke les positions, et un autre qui stocke les normales. Ça revient au même que de mettre tous les attributs dans le même vertex buffer, mais ça peut avoir des performances soit meilleures soit pires, en fonction des situations. C'est une question un peu compliquée dont nous ne soucierons pas, et nous utiliserons simplement ce qui est le plus pratique pour nous.  Enfin, dans data nous mettons nos positions 2D :  .data = { -1.f, -1.f, // Position2D du 1er sommet +1.f, -1.f, // Position2D du 2ème sommet 0.f, +1.f // Position2D du 3ème sommet },   Le système de coordonnées marche ainsi : x et y vont toujours de -1 à 1. Ainsi, x = -1 représente toujours le côté gauche de la fenêtre, et y = 1 représente toujours le haut de la fenêtre. Vous remarquerez donc que quand vous redimensionnez la fenêtre, le triangle se &quot;déforme&quot; pour continuer à remplir toute la fenêtre. Ce n'est généralement pas ce qu'on veut, et nous verrons comment remédier à ça plus tard.  ","version":"Next","tagName":"h3"},{"title":"RenderDoc​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#renderdoc","content":" Avant d'aller plus loin, nous allons commencer à découvrir RenderDoc, un super outil qui nous permettra de débuguer si jamais notre rendu ne se fait pas comme on voudrait.  Remarque Si jamais vous avez besoin de RenderDoc dans Unreal ou Unity, c'est possible ! Vous pouvez ouvrir ces moteurs directement dans RenderDoc, mais même encore mieux il y a un plugin RenderDoc pour Unreal, et le Frame Debugger pour Unity qui est un équivalent de RenderDoc intégré directement dans Unity.  La première chose à faire quand vous ouvrez RenderDoc, c'est d'aller dans l'onglet Launch Application pour choisir l'exécutable à débuguer (qui va se trouver dans le dossier de votre projet, et le sous-dossier build. Il devrait s'appeler TPs-Rendering.exe):    Une fois que s'est fait, vous pouvez cliquer sur Launch :    qui va lancer votre application en &quot;mode RenderDoc&quot; :    Vous pouvez alors faire F12 pour capturer une frame, puis fermer l'application, et RenderDoc va vous permettre d'inspecter la frame que vous venez de capturer :    La première chose à regarder, c'est la timeline sur le côté, qui indique toutes les opérations OpenGL qui ont été exécutées pendant la frame. Vous pouvez cliquer sur chacune d'elle, et voir le rendu à ce moment là de la frame :    Dans notre cas c'est très simple, il y a la couleur de fond qui est faite avec glClear, puis le draw call de notre mesh avec glDrawArrays (le 3 indique qu'il y avait 3 sommets dans notre vertex buffer).  Ensuite on peut inspecter chaque étape du rendu en cliquant dessus. Ce qui va nous intéresser c'est le draw call de notre mesh, donc cliquez sur l'étape glDrawArrays.  La première chose à faire, si typiquement votre mesh ne s'affiche pas, c'est d'aller dans Overlay, et de sélectionner Highlight Drawcall. Ça va indiquer là où votre mesh a atterri à l'écran.    Si vous voyez bien votre mesh en magenta dans le draw call, mais qu'ensuite le triangle ne s'affiche pas, c'est déjà une très bonne information ! Dans ce cas vous pouvez ensuite passer le mode d'overlay en Depth Test, puis Stencil Test, puis Backface Cull. Si l'un d'eux affiche votre mesh en rouge, c'est que cette étape du rasterizer a décidé de skipper votre mesh. Nous reparlerons de ces étapes plus tard (Depth Test, Stencil Test, Backface Culling). Sinon, si tout est vert mais que votre mesh ne s'affiche quand même pas, c'est probablement au niveau du fragment shader qu'il y a un problème.  Si à l'inverse dès l'overlay Highlight Drawcall vous ne voyez pas votre mesh, alors c'est le mesh lui-même qui a un problème. Vous pouvez alors aller dans l'onglet Mesh Viewer :    Ici vous avez plein d'informations : d'abord la vue VS In vous montre votre mesh tel qu'il était avant le vertex shader :    Si votre mesh apparaît bien ici, tant mieux ! Et sinon, c'est probablement que votre vertex buffer n'est pas bon ! Vous pouvez inspecter votre vertex buffer dans la vue VS Input, et regarder si il y a des valeurs bizarres / pas assez de vertexs / un layout qui ne correspond pas à ce que vous pensiez avoir spécifié, etc. Dans cet example, on voit que tout va bien, et on retrouve les valeurs qu'on avait mises dans notre vertex buffer :    Ensuite, vous pouvez aller voir dans VS Out l'état de votre mesh après le vertex shader. Dans les sections suivantes nous appliquerons des transformations au mesh dans le vertex shader (déplacement, caméra, etc.), et vous pourrez voir le résultat ici :    Et une fois de plus, si vous avez besoin d'inspecter les valeurs précises, elles sont toutes dans la vue VS Output :    RenderDoc permet encore beaucoup d'autres choses, mais vous avez vu l'essentiel qui vous sauvera 99% du temps !  ","version":"Next","tagName":"h3"},{"title":"Dessiner un rectangle​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#dessiner-un-rectangle","content":" Maintenant, à vous de jouer ! Modifiez le code qui crée votre mesh afin d'avoir non plus un triangle, mais un rectangle qui prendra la moitié de l'écran. Il vous faudra dessiner deux triangles, et donc indiquer six sommets dans le vertex buffer.  Si vous avez un problème de rendu, pensez à aller voir dans RenderDoc ce qu'il se passe !    ","version":"Next","tagName":"h3"},{"title":"Index buffer​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#index-buffer","content":" Vous avez peut-être remarqué une chose en faisant le vertex buffer du rectangle, c'est que vous avez dû écrire certains sommets deux fois ! (Une fois pour dessiner le premier triangle, puis une deuxième fois pour le deuxième triangle). Et d'ailleurs, six sommets pour un rectangle ça fait beaucoup, 4 devraient suffire ! C'est un problème qui devient d'autant plus embêtant que sur un vrai mesh les sommets sont parfois partagés par bien plus que deux triangles, et donc doivent être dupliqués plein de fois, ce qui augmente considérablement la taille du mesh.  Heureusement, ce problème a une solution : l'index buffer !  Quand on crée notre gl::Mesh on peut, en plus du vertex buffer, spécifier un index buffer :  auto const rectangle_mesh = gl::Mesh{{ .vertex_buffers = {{ .layout = {gl::VertexAttribute::Position2D{0}}, .data = { -0.5f, -0.5f, // Position2D du 1er sommet +0.5f, -0.5f, // Position2D du 2ème sommet +0.5f, +0.5f, // Position2D du 3ème sommet -0.5f, +0.5f // Position2D du 4ème sommet }, }}, .index_buffer = { 0, 1, 2, // Indices du premier triangle : on utilise le 1er, 2ème et 3ème sommet 0, 2, 3 // Indices du deuxième triangle : on utilise le 1er, 3ème et 4ème sommet }, }};   Les indices dans l'index buffer vont par 3, et décrivent un triangle en indiquant quels sommets prendre dans le vertex buffer.    Le rendu est exactement le même, mais notre vertex buffer est plus simple à écrire, et plus léger pour l'ordinateur !  Vous allez pouvoir tester d'écrire vous-même un index buffer quand vous ferez un cube, mais avant cela il nous manque quelques étapes pour passer en 3D !  ","version":"Next","tagName":"h3"},{"title":"Shader​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#shader","content":" Un Shader est un programme exécuté par la carte graphique. Il existe deux principaux types de shaders :  Le Vertex Shader qui prend un sommet dans notre vertex buffer et le modifie (déplacement, application de la perspective 3D, etc.)Le Fragment Shader qui prend un pixel de notre triangle et le colorie (en fonction de son matériau, de la lumière, etc.)  Remarque Il existe aussi les Compute Shaders qui sont plus génériques et prennent n'importe quel type de buffer et le modifient. Ils sont très utilisés pour faire des simulations sur GPU (particules, fluides, vêtements, etc.).  Pour créer un shader, il vous suffit de faire, dans l'initialisation :  auto const shader = gl::Shader{{ .vertex = gl::ShaderSource::File{&quot;res/vertex.glsl&quot;}, .fragment = gl::ShaderSource::File{&quot;res/fragment.glsl&quot;}, }};   et de créer les deux fichiers correspondants, dans le dossier res :  res/vertex.glsl #version 410 layout(location = 0) in vec2 in_position; void main() { gl_Position = vec4(in_position, 0., 1.); }   res/fragment.glsl #version 410 out vec4 out_color; void main() { out_color = vec4(1.); }   IMPORTANT Tous les assets (shader, texture, modèle 3D, etc.) doivent être mis dans le dossier res, et pas un autre, car ce dossier est copié pour être mis à côté de l'exe dans le dossier build. (C'est fait dans le CMakeLists.txt, par la ligne gl_target_copy_folder(${PROJECT_NAME} res)). Si jamais vous vouliez renommer le dossier res ou en ajouter un autre, il faudrait bien penser à aller modifier le CMakeLists.txt !  Puis pour utiliser ce shader à la place du shader par défaut, remplacez la ligne gl::bind_default_shader(); par shader.bind(); :    Le rendu n'a toujours pas changé, car les shaders que je vous ai donnés sont équivalents aux shaders par défaut qu'on utilisait jusque là. Mais ça ne saurait tarder, nous allons maintenant pouvoir modifier nos shaders comme on veut ! Mais avant tout, quelques explications :  ","version":"Next","tagName":"h2"},{"title":"GLSL​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#glsl","content":" Déjà, le GLSL est le langage utilisé pour écrire des shaders. Il ressemble très fort à du C, avec en plus quelques fonctions et types très souvent utilisés en rendu 3D : vecteurs (vec2, vec3, vec4), matrices (mat2, mat3, mat4), et fonctions géométriques (produit scalaire dot(v1, v2), normalisation d'un vecteur normalize(v), etc.).  Chaque shader doit commencer par une indication de la version utilisée (#version 410). Nous utilisons la 410, qui est la dernière supportée par MacOS. (Et sinon elles vont jusqu'à 460, mais il n'y a aucune différence en ce qui nous concerne).  ","version":"Next","tagName":"h3"},{"title":"Vertex Shader​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#vertex-shader","content":" Le vertex shader commence par une déclaration des attributs :  layout(location = 0) in vec2 in_position;   Ces attributs doivent correspondre au layout de notre vertex buffer :  .vertex_buffers = {{ .layout = {gl::VertexAttribute::Position2D{0 /*Index de l'attribut dans le shader*/}}, .data = { // ... }, }},   Le layout(location = 0) correspond à l'index 0 spécifié pour gl::VertexAttribute::Position2D, et le type vec2 vient du fait que nos positions 2D sont des vecteurs à deux composantes (si on utilisait des positions 3D, il faudrait mettre vec3). Le nom in_position est libre et vous pouvez mettre ce que vous voulez.  Ensuite, dans la fonction main() on assigne la variable gl_Position, qui est une variable spéciale indiquant à OpenGL la position finale du vertex (ce qui correspond au VS Out qu'on avait vu dans RenderDoc).  En guise de premier exercice, vous pouvez déplacer le rectangle via le vertex shader, par exemple de 0.4 en x et en y :  astuce Vous ne pouvez pas modifier la variable in_position car elle vient du vertex buffer, et modifier le vertex buffer n'est pas autorisé dans le vertex shader. À la place, créez une copie : #version 410 layout(location = 0) in vec2 in_position; void main() { vec2 position = in_position; // ... // Modifiez `position` comme vous voulez // ... gl_Position = vec4(position, 0., 1.); // Ici on utilise maintenant `position` et non plus `in_position` }     ","version":"Next","tagName":"h3"},{"title":"Fragment Shader​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#fragment-shader","content":" Le fragment shader commence par  out vec4 out_color;   qui déclare la variable de sortie : ce qu'on assigne à cette variable correspondra à la couleur du pixel à l'écran.  Et dans le main() on fait quelque chose de très simple, qui est d'assigner la même couleur à tous les pixels, sans réfléchir :  out_color = vec4(1.);   Note La syntaxe vec4(1.) est un raccourci pour vec4(1., 1., 1., 1.). On peut même faire des choses comme vec4(vec3(0.5), 1.), qui revient à faire vec4(0.5, 0.5, 0.5, 1.).Les composantes R, G, B et A des couleurs vont de 0 à 1, donc vec3(1., 1., 1.) correspond à mettre le maximum de rouge, vert et bleu, donc du blanc pur.  En guise de premier exercice, vous pouvez changer la couleur du rectangle :    ","version":"Next","tagName":"h3"},{"title":"Envoyer des paramètres au shader : les uniforms​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#envoyer-des-paramètres-au-shader--les-uniforms","content":" Pour que nos shaders deviennent intéressants, il faut leur envoyer plus de données en entrée. On peut soit rajouter des attributs dans le vertex buffer (couleur, UV, normale, etc.), ce que nous verrons plus tard ; soit envoyer des paramètres appelés uniforms. Une variable uniforme se déclare ainsi dans le shader, au-dessus du main() :  uniform vec2 nom_de_votre_variable_uniforme; // Vous pouvez mettre le type que vous voulez, et le nom que vous voulez   puis peut s'utiliser dans votre shader comme vous le voulez, comme n'importe quelle variable normale.  Pour assigner la valeur d'une variable uniforme, cela se fait dans votre code C++, après avoir bind le shader :  shader.set_uniform(&quot;nom_de_votre_variable_uniforme&quot;, glm::vec2{1.f, 3.f});   Remarque On utilise la librairie glm pour avoir des types vecteur et matrice comme en glsl : glm::vec2, glm::vec3, glm::vec4, glm::mat2, glm::mat3, glm::mat4, etc.  Note Une variable uniform s'appelle ainsi car elle est uniforme pour un draw call : ça sera la même pour tous les vertexs et pour tous les pixels lors d'un appel donné à mesh.draw(). Si on voulait une valeur qui est différente pour chaque vertex, il faudrait passer par un attribut de vertex.  On peut par exemple utiliser les uniforms pour régler notre problème de taille de rectangle qui suit la fenêtre. Pour cela, nous allons passer au shader l'aspect ratio de la fenêtre (i.e. largeur / hauteur), et corriger la position en x de nos vertexs en fonction du ratio.  Déclarez une uniform aspect_ratio de type float dans le vertex shaderDivisez le x de la position de votre vertex par aspect_ratioCôté C++, passez la uniform au shader (Vous pouvez obtenir l'aspect ratio de la fenêtre avec gl::framebuffer_aspect_ratio())  Enfin un carré qui reste carré peu importe la taille de la fenêtre !  Remarque On pourrait se dire qu'il suffisait de créer un nouveau vertex buffer avec des positions qui prennent en compte l'aspect ratio, et de recréer le buffer à chaque fois que l'aspect ratio change. Mais ça impliquerait de modifier potentiellement les millions de vertexs de notre mesh, ce qui prendrait beaucoup de temps. Alors que le vertex shader lui va faire ça en un rien de temps : c'est la puissance de la carte graphique, qui peut traiter tous les sommets en parallèle extrêmement vite !  ","version":"Next","tagName":"h3"},{"title":"Exercice : Faire bouger le carré​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#exercice--faire-bouger-le-carré","content":" Vous pouvez utiliser gl::time_in_seconds() pour récupérer le temps, l'envoyer au shader, et vous en servir pour faire bouger le carré. Le plus simple est de le faire aller en ligne droite, mais vous pouvez aussi (bonus) le faire aller et revenir, ou tourner en rond :     Remarque Maintenant qu'on a un objet qui bouge, on peut enfin tester ce qu'il se passe quand on enlève la ligne glClear(GL_COLOR_BUFFER_BIT);. Je vous laisse essayer !  ","version":"Next","tagName":"h3"},{"title":"Bonus : effet de fade​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#bonus--effet-de-fade","content":" Vous avez toutes les cartes en main pour faire cet effet de fade, alors je vous laisse chercher comment faire 😉  Info Vous aurez probablement besoin d'utiliser de la transparence à un moment, qui nécessite d'être activée, au début de l'initialisation, avec : glEnable(GL_BLEND); glBlendFuncSeparate(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA, GL_ONE_MINUS_DST_ALPHA, GL_ONE); // On peut configurer l'équation qui mélange deux couleurs, comme pour faire différents blend mode dans Photoshop. Cette équation-ci donne le blending &quot;normal&quot; entre pixels transparents.     Remarque Si vous voyez un effet de clignotement, c'est normal, c'est dû à la swapchain : en fait il y a deux images qui s'alternent : l'une qui est affichée à l'écran, et l'autre sur laquelle on est en train de dessiner. (Si on dessinait sur l'image qui est actuellement affichée à l'écran, on verrait les pixels se dessiner petit à petit et ça ferait très moche). Pour résoudre ce clignotement, il faudrait faire le rendu de toute notre scène dans une render target à part, qu'on copierait à l'écran à la fin de chaque frame. Nous verrons cette notion plus tard.  Remarque Il reste une trace qui ne s'efface pas, c'est dû à des problèmes d'arrondi au moment du calcul de la transparence, car chaque canal de couleur est stocké sur un entier à 8 bits seulement (par défaut). En faisant notre rendu sur une render target utilisant 16 ou 32 bits par canal, ça résoudrait le problème.  Remarque Cet effet dépend du framerate ! Si vous dessinez deux fois plus d'images par seconde, la trace va s'effacer deux fois plus vite. Pour éviter cela, il faudrait prendre en compte gl::delta_time_in_seconds(), qui donne le temps écoulé entre deux frames.  ","version":"Next","tagName":"h3"},{"title":"Caméra et Matrices​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#caméra-et-matrices","content":" Il est temps de passer en 3D !  Pour cela nous avons besoin de deux informations :  Le point de vue, i.e. savoir où on est dans l'espace, et dans quelle direction on regardeLa projection, pour donner l'effet de perspective inhérent à la 3D  Pour représenter ces deux informations, nous allons utiliser des matrices. Une matrice est un objet mathématique qui permet de représenter une transformation géométrique. On applique une matrice à un vecteur (point ou direction, en 2D ou en 3D) pour lui appliquer la transformation géométrique représentée par la matrice. Par exemple on peut créer une matrice de rotation qui, quand elle est appliquée à un point, fait tourner ce point.  Remarque Une matrice est un grille de nombre. Par exemple une matrice 3D (mat3) ressemblerait à : (104012001)\\begin{pmatrix} 1 &amp; 0 &amp; 4 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}​100​010​421​​ Je ne rentrerai pas dans les détails de quels nombres mettre dans la matrice pour représenter quelle transformation, car nous allons utiliser la librairie glm pour créer toutes ces matrices.  Le gros intérêt des matrices est qu'on peut les combiner entre elles en les multipliant ! Par exemple si j'ai une matrice 3D de rotation R et une matrice 3D de translation T, alors R * T est une nouvelle matrice 3D, dont la transformation géométrique est une translation suivie d'une rotation (NB : la transformation de droite est appliquée en première).  On peut ainsi construire une seule matrice finale, représentant la combinaison du point de vue (view matrix) et de la projection (projection matrix). On peut aussi y rajouter une modification de notre mesh (pour faire tourner le mesh, le translater pour le positionner dans le monde où on veut, etc.) (model matrix). Ensuite on envoie cette seule matrice au shader, il l'applique à la position de nos vertexs, et le tour est joué !  Attention L'ordre des matrices a son importance ! R * T est différent de T * R. T * R est une rotation suivie d'une translation, ce qui est différent de d'abord faire la translation puis la rotation, comme vous pourrez le constater dans l'exercice qui suit.  Remarque On ne peut pas représenter n'importe quelle transformation géométrique avec des matrices, seulement celles qui sont affines. Mais c'est déjà bien assez, car les translations, les rotations et les aggrandisements sont toutes des transformations affines. Vous pouvez tester cette démo interactive pour voir quel genre de transformation géométrique une matrice peut produire, et aussi regarder cette excellentissime vidéo.  ","version":"Next","tagName":"h2"},{"title":"View Matrix​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#view-matrix","content":" Pour obtenir la View Matrix nous allons utiliser une caméra, que nous pourrons contrôler pour se déplacer dans le monde :  // Dans l'initialisation auto camera = gl::Camera{};   puis, pour que la caméra puisse réagir aux évènements (clic, déplacement de la souris, etc.), il faut la connecter aux évènements fournis par la librairie gl :  // Dans l'initialisation gl::set_events_callbacks({camera.events_callbacks()});   Une fois que c'est fait, on peut récupérer la matrice de vue avec :  // À chaque frame glm::mat4 const view_matrix = camera.view_matrix();   Remarque gl::set_events_callbacks() prend un tableau de callbacks, donc on pourrait rajouter nos propres callbacks en plus de ceux de la caméra : gl::set_events_callbacks({ camera.events_callbacks(), { .on_mouse_pressed = [&amp;](gl::MousePressedEvent const&amp; e) { std::cout &lt;&lt; &quot;Mouse pressed at &quot; &lt;&lt; e.position.x &lt;&lt; &quot; &quot; &lt;&lt; e.position.y &lt;&lt; '\\n'; }, }, });   ","version":"Next","tagName":"h3"},{"title":"Projection Matrix​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#projection-matrix","content":" Il nous faut encore la matrice de projection. Celle-ci est plus simple et s'obtient directement grâce à glm :  glm::mat4 const projection_matrix = glm::infinitePerspective(1.f /*field of view in radians*/, gl::framebuffer_aspect_ratio() /*aspect ratio*/, 0.001f /*near plane*/);   (Attention, il faudra inclure le bon fichier de glm au début de main.cpp : #include &quot;glm/ext/matrix_clip_space.hpp&quot;)  Ses différents paramètres sont :  Le field of view (angle de vue). Vous avez peut-être déjà vu ce paramètre dans des jeux vidéos. Plus il est large, plus on voit une grande partie de la scène à la fois (attention, avec des valeurs trop grande les objets commencent à apparaître déformés), plus il est petit moins on voit une grande portion de la scène, ça zoom. Attention, il est exprimé en radians, donc pour un fov de 45° il faudra écrire glm::radians(45.f).L'aspect ratio de la fenêtre. La petite division par aspect_ratio qu'on avait fait précédemment pour corriger notre problème de stretch est gérée automatiquement par la matrice de projection, vous pouvez donc enlever cette ligne du vertex shader.Le near plane : à cause de limitations techniques, les objets trop proches de la caméra ne peuvent pas être visibles. Le near plane définit à partir de quelle distance on commence à voir les objets. Plus on le met proche de 0, plus on évitera d'avoir des objets coupés, mais si on le met trop petit on peut commencer à avoir des erreurs d'arrondis dans nos calculs entre float, ce qui causerait d'autres problèmes dans notre rendu.Le far plane : on n'en a pas ici car on utilise glm::infinitePerspective(), mais si on utilisait glm::perspective() on en aurait un. Similaire au near plane, il définit la distance à partir de laquelle on ne voit plus les objets.  Remarque Il y a aussi un autre type de projection : la projection orthographique. Elle ne fait pas intervenir de perspective, donc les objets lointains apparaissent à la même taille que les objets proches. Ce n'est pas réaliste, mais peut être intéressant pour donner un style au rendu.  ","version":"Next","tagName":"h3"},{"title":"Envoyer au shader​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#envoyer-au-shader","content":" Maintenant qu'on a ces deux matrices, on peut les multiplier entre elles pour former la view_projection_matrix (⚠ Attention, la view doit être celle qui va s'appliquer en premier au vecteur, et la projection en deuxième ! Réfléchissez donc bien à l'ordre dans lequel vous multipliez vos matrices), et envoyer cette view_projection matrix au shader (déclarez une uniform de type mat4). Enfin, il ne reste plus qu'à multiplier dans le shader la matrice à la position de nos vertexs :  gl_Position = view_projection_matrix * vec4(in_position, 0., 1.);   Remarque Nos matrices sont des mat4 alors qu'on est en 3D. C'est parce qu'on utilise une &quot;astuce&quot; qui sont les coordonnées homogènes, et sans lesquelles on ne pourrait pas faire de translation ni de perspective. C'est pour ça qu'on a besoin de rajouter une coordonnée de plus que la dimension de l'espace. C'est aussi pour ça qu'on rajoute un 1 en quatrième coordonnée de nos positions (vec4(in_position, 0., 1.)).    ","version":"Next","tagName":"h3"},{"title":"Exercice : paramètres de la projection​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#exercice--paramètres-de-la-projection","content":" Essayez de changer les paramètres de la matrice de projection (field of view, near plane) et essayez d'observer la différence de rendu. Essayez aussi d'utiliser une projection orthographique. (Je vous laisse chercher en ligne comment on fait ça avec glm).  ","version":"Next","tagName":"h3"},{"title":"Exercice : model matrix​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#exercice--model-matrix","content":" On peut rajouter un troisième matrice à la view_projection_matrix, pour former la model_view_projection_matrix ! (Attention, la model matrix doit s'appliquer en premier).  Avec glm vous pouvez créer une matrice de rotation avec  glm::mat4 const rotation = glm::rotate(glm::mat4{1.f}, gl::time_in_seconds() /*angle de la rotation*/, glm::vec3{0.f, 0.f, 1.f} /* axe autour duquel on tourne */);   et une matrice de translation avec  glm::mat4 const translation = glm::translate(glm::mat4{1.f}, glm::vec3{0.f, 1.f, 0.f} /* déplacement */);   (Il faudra include #include &quot;glm/ext/matrix_transform.hpp&quot;.)  Créez une matrice modèle qui combine une translation et une rotation, et observez le résultat. Essayez les deux ordres (rotation suivie de translation, et vice-versa) et vous verrez que ce n'est pas pareil ! L'ordre a son importance !  ","version":"Next","tagName":"h3"},{"title":"Cube​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#cube","content":" Maintenant qu'on peut voir en 3D, il est temps de faire notre premier mesh 3D ! Faites le vertex buffer et l'index buffer pour un cube. Je vous laisse revoir le chapitre dédié au besoin. Je vous conseille de faire des petits schémas pour ne pas vous y perdre dans les indices, et y aller face par face.  Attention Pensez à modifier aussi votre shader, pour qu'il reçoive une position 3D et plus 2D.    ","version":"Next","tagName":"h2"},{"title":"Premier shader pour mieux voir la 3D​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#premier-shader-pour-mieux-voir-la-3d","content":" On a du mal à discerner la 3D sur notre cube monochrome. Nous allons donc changer notre fragment shader pour commencer à un peu mieux voir tout ça.  Une manière très simple va être de passer la position des vertexs du vertex shader vers le fragment shader, puis d'utiliser ces positions comme des couleurs, afin que chaque vertex ait une couleur différente.  Pour cela dans le vertex shader nous allons déclarer une variable out, qui sera transmise au fragment shader automatiquement :  res/vertex.glsl // À mettre avant le main out vec3 vertex_position;   puis on l'assigne dans le main :  res/vertex.glsl vertex_position = in_position;   et ensuite dans le fragment shader on déclare un variable in avec le même nom et le même type que la variable out de notre vertex shader :  res/fragment.glsl // À mettre avant le main in vec3 vertex_position;   et on peut l'utiliser dans notre main :  res/fragment.glsl out_color = vec4(vertex_position, 1.);     Si votre cube vous paraît un peu bizarre, c'est normal, il nous manque encore un Depth Buffer pour faire de la 3D correctement !  Note Vous avez peut-être remarqué qu'il y a un dégradé de couleurs. C'est parce que, quand on passe une variable in / out entre le vertex shader et le fragment shader, elle est interpolée pour chaque pixel (en faisant une moyenne de la valeur aux trois sommets du triangle contenant le pixel).  ","version":"Next","tagName":"h2"},{"title":"Depth Buffer​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#depth-buffer","content":" Le problème avec notre rendu pour l'instant, c'est que les triangles se dessinent les uns après les autres et se recouvrent. Et si par malchance c'est une face arrière du cube qui est dessinée en dernière, alors elle va venir cacher les faces avant qui ont été dessinées avant.  Pour remédier à ça, il faut activer le Depth Test :  // À mettre dans l'initialisation glEnable(GL_DEPTH_TEST);   et clear le depth buffer à chaque frame, tout comme on clear la couleur de l'écran :  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Vient remplacer glClear(GL_COLOR_BUFFER_BIT);     Et voilà ! Notre premier rendu 3D qui ressemble à peu près à quelque chose ! 🎉  Et qu'est-ce donc qu'un Depth Buffer au fait ? C'est une deuxième image, qui se crée en parallèle de la couleur qu'on met à l'écran, et qui stocke la profondeur correspondant à chaque pixel. Ainsi chaque triangle dessine à la fois une couleur à l'écran (contrôlée par le fragment shader), et aussi une &quot;couleur&quot; dans le depth buffer. Et avant même de dessiner, on vérifie pour chaque pixel si il n'y avait pas déjà eu quelque chose de dessiné sur ce pixel, et si oui on compare leur profondeur, et on recolorie le pixel avec la nouvelle couleur seulement si il est plus proche de la caméra que l'objet précédemment dessiné (on obtient cette distance à la caméra justement en allant lire le depth buffer).  On peut aller visualiser notre Depth Buffer dans RenderDoc :    Dans l'onglet Outputs il y a maintenant deux images : l'écran normal (Backbuffer Color), et le Depth Buffer (Backbuffer Depth-stencil). (NB : pour y voir quelque chose dans le depth buffer, il faut changer la Range, car comme le cube est très proche il apparaît très blanc dans le depth buffer).  Remarque On verra un usage créatif du depth buffer pour faire un effet de rendu See-Through.  ","version":"Next","tagName":"h2"},{"title":"Texture​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#texture","content":" ","version":"Next","tagName":"h2"},{"title":"UV​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#uv","content":" Nous allons maintenant appliquer une texture à notre objet ! Pour cela, il nous faut tout d'abord des coordonnées de texture, qui vont indiquer quelle partie de la texture s'applique à quelle partie du mesh. Pour l'instant revenez à un mesh de carré, et rajoutez un vertex attribute de type UV. Les coordonnées de texture sont souvent appelées UV (U pour l'axe X, et V pour l'axe Y), et vont de 0 à 1. Pour vérifier que vos UVs sont bien placés, vous pouvez les afficher comme une couleur dans le fragment shader: out_color = vec4(uv.x, uv.y, 0., 1.);. C'est une technique très souvent utilisée pour débuguer sur GPU, comme on n'a pas de print() pour afficher des valeurs, on affiche des couleurs et on les interprète comme des nombres. Le rouge correspond à uv.x (0 à gauche et 1 à droite), et le vert à uv.y (0 en bas et 1 en haut). Ça doit ressembler à ça :    ","version":"Next","tagName":"h3"},{"title":"Objet Texture​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#objet-texture","content":" Une fois qu'on a nos UVs, on peut maintenant créer notre objet texture :  auto const texture = gl::Texture{ gl::TextureSource::File{ // Peut être un fichier, ou directement un tableau de pixels .path = &quot;res/texture.png&quot;, .flip_y = true, // Il n'y a pas de convention universelle sur la direction de l'axe Y. Les fichiers (.png, .jpeg) utilisent souvent une direction différente de celle attendue par OpenGL. Ce booléen flip_y est là pour inverser la texture si jamais elle n'apparaît pas dans le bon sens. .texture_format = gl::InternalFormat::RGBA8, // Format dans lequel la texture sera stockée. On pourrait par exemple utiliser RGBA16 si on voulait 16 bits par canal de couleur au lieu de 8. (Mais ça ne sert à rien dans notre cas car notre fichier ne contient que 8 bits par canal, donc on ne gagnerait pas de précision). On pourrait aussi stocker en RGB8 si on ne voulait pas de canal alpha. On utilise aussi parfois des textures avec un seul canal (R8) pour des usages spécifiques. }, gl::TextureOptions{ .minification_filter = gl::Filter::Linear, // Comment on va moyenner les pixels quand on voit l'image de loin ? .magnification_filter = gl::Filter::Linear, // Comment on va interpoler entre les pixels quand on zoom dans l'image ? .wrap_x = gl::Wrap::Repeat, // Quelle couleur va-t-on lire si jamais on essaye de lire en dehors de la texture ? .wrap_y = gl::Wrap::Repeat, // Idem, mais sur l'axe Y. En général on met le même wrap mode sur les deux axes. } };   Si vous voulez, vous pouvez télécharger cette texture de test.  Nous testerons les différentes TextureOptions juste après, mais déjà pour afficher la texture il ne vous reste plus qu'à :  Déclarer une variable uniforme de type sampler2D dans le fragment shader. (Un sampler est l'objet qui nous permet d'accéder à la texture en appliquant les TextureOptions qu'on a choisies) : uniform sampler2D my_texture;Passer la texture au shader via une variable uniforme : shader.set_uniform(&quot;my_texture&quot;, texture);De nouveau dans le fragment shader, lire la texture à la position uv grâce à la fonction texture() de glsl : vec4 texture_color = texture(my_texture, uv); et la retourner en sortie du fragment shader.  Et voilà !  ","version":"Next","tagName":"h3"},{"title":"Options de texture​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#options-de-texture","content":" Wrap​  Le mode de wrapping contrôle ce qu'il se passe quand on passe des UVs à la fonction texture() qui ne sont pas entre 0 et 1. Plutôt que de crash comme le ferait un tableau normal quand on utilise un index invalide (&lt; 0 ou &gt;= size), la texture va quand même retourner une couleur. Pour tester ces différents modes, utilisez des UVs qui vont de -1 à 2, et testez tous les modes ! Vous verrez que les noms sont plutôt explicites. (PS : pour le mode ClampToBorder il faut spécifier le paramètre border_color en plus dans TextureOptions).  gl::Wrap::MirroredRepeat  Filter​  Le filtre de magnification contrôle ce qu'il se passe quand on zoome dans l'image. Pour en visualiser l'effet, utilisez des UVs qui vont de 0.8 à 0.9 par exemple.  Note Pour le filtre LinearMipmapLinear il faut avoir créé des mipmaps pour votre texture, nous en parlerons plus tard.  gl::Filter::NearestNeighbour  Le filtre de magnification est utilisée quand la texture est vue de loin et couvre &quot;peu&quot; de pixels à l'écran (moins que le nombre de pixels dans la texture).  L'effet du filtre est beaucoup moins visible, et c'est surtout quand il y a du mouvement que le filtre Linear peut éviter un peu de flicker. Vous pouvez le voir en utilisant une texture d'échiquier, des UVs entre 0 et 30, et en faisant déplacer la texture à une vitesse de 0.0001 par seconde.  ","version":"Next","tagName":"h3"},{"title":"Bonus : textures procédurales​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#bonus--textures-procédurales","content":" Il est aussi possible, au lieu de lire une texture, de colorier un triangle en calculant une &quot;texture&quot; procédurale en fonction des UVs. On en a déjà vu un exemple très simple quand on affiché nos UVs (out_color = vec4(uv.x, uv.y, 0., 1.);), mais on peut faire beaucoup plus ! Par exemple toutes les images sur Shadertoy sont faites ainsi, juste en affichant un quad sur tout l'écran, et en faisant des calculs élaborés dans le fragment shader.    Pour commencer simplement, vous pouvez vous demander comment produire un disque, ou un pattern d'échiquier :  \t  Pour aller plus loin, je vous recommande les excellentes vidéos de The Art of Code :    ","version":"Next","tagName":"h3"},{"title":"Cube texturé​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#cube-texturé","content":" Vous pouvez maintenant reprendre votre mesh de cube, et lui rajouter des UVs pour pouvoir appliquer une texture. Vous remarquerez qu'on ne peut pas avoir la texture qui s'affiche bien sur les 6 faces à la fois. Du moins tant qu'on n'utilise que 8 vertexs. En effet, utiliser un index buffer c'est bien pratique, mais parfois on a besoin de dupliquer nos sommets afin d'avoir un attribut différent pour chaque face même si elles partagent un même sommet. Par exemple ici nos sommets partagent la même position, mais pas les mêmes UVs en fonction de la face qu'on considère. Ne vous embêtez pas à le faire, mais sachez que pour résoudre ce problème dans le cas de notre cube il faudrait se passer d'un index buffer, et re-préciser les sommet une fois pour chaque triangle, comme on le faisait au début. (Donc 36 sommets dans le cas de notre cube !)    ","version":"Next","tagName":"h3"},{"title":"Render Target​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#render-target","content":" Pour finir ce long chapitre sur les différents objets fondamentaux des moteurs de rendu 3D, nous allons parler des Render Targets ! (Aussi appelées Framebuffers).  Une Render Target sert à faire notre rendu sur une texture à part, au lieu de le faire directement à l'écran. Ça peut être très utile quand on a besoin de réutiliser l'image produite, par exemple pour appliquer du post-processing dessus.  Commencez par créer une Render Target : vous reconnaîtrez certains paramètres qu'on a déjà utilisés en créant une texture :  auto render_target = gl::RenderTarget{gl::RenderTarget_Descriptor{ .width = gl::framebuffer_width_in_pixels(), .height = gl::framebuffer_height_in_pixels(), .color_textures = { gl::ColorAttachment_Descriptor{ .format = gl::InternalFormat_Color::RGBA8, .options = { .minification_filter = gl::Filter::NearestNeighbour, // On va toujours afficher la texture à la taille de l'écran, .magnification_filter = gl::Filter::NearestNeighbour, // donc les filtres n'auront pas d'effet. Tant qu'à faire on choisit le moins coûteux. .wrap_x = gl::Wrap::ClampToEdge, .wrap_y = gl::Wrap::ClampToEdge, }, }, }, .depth_stencil_texture = gl::DepthStencilAttachment_Descriptor{ .format = gl::InternalFormat_DepthStencil::Depth32F, .options = { .minification_filter = gl::Filter::NearestNeighbour, .magnification_filter = gl::Filter::NearestNeighbour, .wrap_x = gl::Wrap::ClampToEdge, .wrap_y = gl::Wrap::ClampToEdge, }, }, }};   En général une Render Target a plusieurs textures (qu'on appelle des attachments) : au moins une texture de couleur1, et (optionnellement) une texture de profondeur (le fameux Depth Buffer) (qui peut aussi contenir un Stencil Buffer dont nous reparlerons plus tard).  Puisque dans notre cas on va utiliser la render target pour faire du post-processing, on veut que la texture aie la même taille que l'écran. C'est pourquoi on l'initialise avec  .width = gl::framebuffer_width_in_pixels(), .height = gl::framebuffer_height_in_pixels(),   mais il faut également la changer si la fenêtre est redimensionnée ! Rajoutez cet event callback :  gl::set_events_callbacks({ camera.events_callbacks(), {.on_framebuffer_resized = [&amp;](gl::FramebufferResizedEvent const&amp; e) { if(e.width_in_pixels != 0 &amp;&amp; e.height_in_pixels != 0) // OpenGL crash si on tente de faire une render target avec une taille de 0 render_target.resize(e.width_in_pixels, e.height_in_pixels); }}, });   Maintenant que notre render target est créée, on peut dessiner dessus. Pour cela, il suffit d'appeler sa méthode render() et de lui passer un callback contenant du code de dessin normal. Ces draw calls dessineront non pas à l'écran, mais sur notre render target !   render_target.render([&amp;]() { glClearColor(1.f, 0.f, 0.f, 1.f); // Dessine du rouge, non pas à l'écran, mais sur notre render target glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // ... mettez tout votre code de rendu ici });   En faisant ça on ne voit plus rien à l'écran, et c'est normal car tout est maintenant mis sur notre render target à la place. Nous allons l'afficher dans un instant, mais en attendant on peut déjà aller la voir dans RenderDoc et confirmer que le rendu s'est bien fait :  Notre cube est toujours là, mais au lieu d'être rendu à l'écran (qui s'appelait Backbuffer Color dans notre capture RenderDoc), la texture d'output est maintenant Texture 54.  Maintenant pour afficher notre texture, il nous suffit de dessiner un quad sur tout l'écran, de passer la texture à un shader, et de la lire comme au chapitre sur les textures :    Et voilà ! On est revenu au point de départ avec notre cube, mais l'avantage c'est qu'on peut maintenant manipuler la texture comme on veut dans un shader et appliquer plein d'effets !  ","version":"Next","tagName":"h2"},{"title":"Post-Process​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#post-process","content":" Dans le fragment shader qui lit la texture de la render target, on peut maintenant manipuler la couleur comme on veut ! Essayez de tout passer en noir et blanc, ou de ne garder que la composante rouge de l'image :  \t  Bonus : Vous pouvez aussi tenter plein d'autres effets, comme augmenter le contraste ou la saturation de l'image, faire du vignettage, déformer l'image, etc.  ","version":"Next","tagName":"h3"},{"title":"Bonus : Fade​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#bonus--fade","content":" Utiliser une render target peut aussi permettre de choisir le format (e.g. RGBA8) de la texture sur laquelle on rend, au lieu d'être limité à la valeur choisie par défaut par OpenGL. Reprenez votre code du fade, et faites maintenant le rendu sur une render target utilisant un format avec 16 ou 32 bits par canal au lieu de 8, ça va régler les problèmes de précision qu'on avait. Et le fait d'utiliser une render target règle aussi le problème de swapchain qui faisait clignoter l'image.    ","version":"Next","tagName":"h3"},{"title":"Autres utilisations​","type":1,"pageTitle":"Partie 2 - Bases du rendu","url":"/Rendering/M1 GP/Partie 2 - Bases du rendu#autres-utilisations","content":" Note Sur ce schéma du pipeline de rendu d'Unreal, la plupart des étapes intermédiaires stockent leur résultat dans une render target, qui sont ensuite réutilisées par d'autres étapes. Ça vous donne une idée du nombre de render targets utilisées !  Footnotes​ Il peut y en avoir plusieurs. Ça permet de dessiner sur plusieurs textures en même temps avec un seul draw call. Pour cela il suffit dans le fragment shader de déclarer plusieurs variables out: layout(location = 0) out vec4 out_color1; layout(location = 1) out vec4 out_color2; et d'écrire dans chacune d'elles. ↩ ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}